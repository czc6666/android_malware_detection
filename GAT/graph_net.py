import matplotlib

matplotlib.use('agg')  # 使用agg后端，避免在服务器上绘图
import numpy as np  # 矩阵运算
import time
import torch
import torch.utils  # 数据加载器
import torch.utils.data  # 数据加载器
import torch.nn.functional as F  # 激活函数
import torch.optim as optim  # 优化器
import torch.optim.lr_scheduler as lr_scheduler  # 学习率调度器
from torch.utils.data import DataLoader  # 数据加载器
from .GraphData import GraphData  # 图数据
from .DataReader_GAT import DataReader  # 数据读取器
from .DataReader_GCN import DataReader as DataReaderGCN  # 数据读取器
# from .model import GAT, GCN  #, GraphUnet, MGCN
from .model import GAT, GCN  #, GraphUnet, MGCN
import sys

from czc.memory_usage import memory_usage  # 内存使用

print('torch版本: ', torch.__version__)  # 打印torch版本
class graph_net:
    args = {}

    def __init_args__(self):
        from .model import LayerType  # 导入LayerType，用于图层类型
        self.args['description'] = 'Graph Convolutional Networks'  # 描述
        self.args['dataset'] = 'AMD_AndroZoo_test'  # 数据集
        self.args['model'] = 'gat'  # 模型选择，choices=['gcn', 'gat']
        self.args['lr'] = 0.001  # 学习率
        self.args['lr_decay_steps'] = '5,10,15,20,25,30,35,40,45'  #,50,55,60,65,70,75,80,85,90,95
        self.args['wd'] = 1e-4  # 权重衰减
        self.args['dropout'] = 0.2  # dropout率
        self.args['filters'] = '128'  # 每层过滤器数量
        self.args['filter_scale'] = 1  # 过滤器比例（感受野大小），必须大于0；1为GCN，大于1为ChebNet
        self.args['n_hidden'] = 0 # 全连接层在最后一层卷积层后的隐藏单元数量
        self.args['n_hidden_edge'] = 32 # 边预测网络中全连接层的隐藏单元数量
        self.args['epochs'] = 10 # 训练轮数
        self.args['batch_size'] = 10 # 批量大小
        self.args['bn'] = False # 使用批归一化层
        self.args['threads'] = 0 # 加载数据线程数
        self.args['log_interval'] = 10 # 日志间隔（批次数）
        self.args['device'] = 'cuda' # 设备选择，choices=['cuda', 'cpu']
        self.args['seed'] = 1 # 随机种子
        self.args['shuffle_nodes'] = False # 调试时打乱节点
        self.args['adj_sq'] = False # 使用A^2代替A作为邻接矩阵
        self.args['scale_identity'] = False # 使用2I代替I作为自连接
        self.args['visualize'] = False # 仅用于unet：保存一些邻接矩阵和其他数据为图像
        self.args['use_cont_node_attr'] = True # 使用连续节点属性
        self.args['use_node_labels'] = False # 使用节点标签
        self.args['num_of_layers'] = 1 # GAT专用，表示GAT层数
        self.args['num_heads_per_layer'] = [2] # GAT专用
        self.args['num_features_per_layer'] = [500, 128] # GAT专用，表示每层的特征数量，相当于GCN的filters
        self.args['add_skip_connection'] = True # GAT专用
        self.args['bias'] = True # GAT专用
        self.args['layer_type'] = LayerType.IMP2 # GAT专用
        self.args['log_attention_weights'] = False # GAT专用

        self.args['filters'] = list(map(int, self.args['filters'].split(',')))  # 每层过滤器数量
        self.args['lr_decay_steps'] = list(map(int, self.args['lr_decay_steps'].split(',')))  # 学习率衰减步数

    def __init__(self, root, dataset):  # 初始化
        self.__init_args__()
        self.args['dataset'] = dataset  # 数据集    
        self.root = root  # 数据目录
        # for arg in self.args:
        #     print(arg, self.args[arg])
        torch.backends.cudnn.deterministic = True  # 确定性
        torch.backends.cudnn.benchmark = True  # 优化
        torch.manual_seed(self.args['seed'])  # 随机种子
        torch.cuda.manual_seed(self.args['seed'])  # 随机种子   
        torch.cuda.manual_seed_all(self.args['seed'])  # 随机种子
        self.rnd_state = np.random.RandomState(self.args['seed'])  # 随机种子


    def collate_batch(self, batch):
        '''
        通过对节点特征和邻接矩阵进行零填充（zero-padding）来创建相同大小的图批次（batch）。
        填充的大小取决于当前批次中的最大节点数，而不是整个数据集中的最大节点数。
        批次中的图通常比数据集中最大的图小得多，所以这种方法效率较高。
        参数：
            batch: PyTorch Geometric 格式的批次，或者是 [节点特征*批次大小, 邻接矩阵*批次大小, 标签*批次大小] 的列表
        返回值：
            [节点特征, 邻接矩阵, 图支持矩阵, 节点数量, 标签]
        。
        Creates a batch of same size graphs by zero-padding node features and adjacency matrices up to
        the maximum number of nodes in the CURRENT batch rather than in the entire dataset.
        Graphs in the batches are usually much smaller than the largest graph in the dataset, so this method is fast.
        :param batch: batch in the PyTorch Geometric format or [node_features*batch_size, A*batch_size, label*batch_size]
        :return: [node_features, A, graph_support, N_nodes, label]
        '''
        # assert len(batch) == 1, str(len(batch)) + 'batch must be 1!'
        B = len(batch)
        # print('--------------------------------------------------------', len(batch[0]))
        N_nodes = [len(batch[b][1]) for b in range(B)]
        C = batch[0][0].shape[1]
        N_nodes_max = int(np.max(N_nodes))

        graph_support = torch.zeros(B, N_nodes_max)
        # A = torch.zeros(B, N_nodes_max, N_nodes_max)
        A = torch.full((B, N_nodes_max, N_nodes_max), -1e9)
        # A = torch.tensor(B*[ N_nodes_max * [N_nodes_max * [-1e9]]])
        for i in range(B):
            for j in range(N_nodes_max):
                A[i, j, j] = 0
        # print(A.shape)
        x = torch.zeros(B, N_nodes_max, C)
        graph_feature = torch.zeros(B, batch[0][3].shape[0])
        for b in range(B):
            x[b, :N_nodes[b]] = batch[b][0]
            A[b, :N_nodes[b], :N_nodes[b]] = batch[b][1]
            graph_support[b][:N_nodes[b]] = 1  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1
            graph_feature[b] = batch[b][3]

        # for i in range(B):
        #     for j in range(N_nodes_max):
        #         for k in range(N_nodes_max):
        #             A[i, j, k] = -1e9 * (1.0 - A[i, j, k])

        # print('done')
        N_nodes = torch.from_numpy(np.array(N_nodes)).long()
        labels = torch.from_numpy(np.array([batch[b][2] for b in range(B)])).long()

        return [x, A, graph_support, N_nodes, labels, graph_feature]

    def collate_batch_gcn(self, batch):
        '''
        Creates a batch of same size graphs by zero-padding node features and adjacency matrices up to
        the maximum number of nodes in the CURRENT batch rather than in the entire dataset.
        Graphs in the batches are usually much smaller than the largest graph in the dataset, so this method is fast.
        :param batch: batch in the PyTorch Geometric format or [node_features*batch_size, A*batch_size, label*batch_size]
        :return: [node_features, A, graph_support, N_nodes, label]
        '''
        # assert len(batch) == 1, str(len(batch)) + 'batch must be 1!'
        # raise Exception

        B = len(batch)
        # print('--------------------------------------------------------', len(batch[0]))
        N_nodes = [len(batch[b][1]) for b in range(B)]
        C = batch[0][0].shape[1]
        N_nodes_max = int(np.max(N_nodes))

        graph_support = torch.zeros(B, N_nodes_max)
        A = torch.zeros(B, N_nodes_max, N_nodes_max)
        # A = torch.full((B, N_nodes_max, N_nodes_max), -1e9)
        # A = torch.tensor(B*[ N_nodes_max * [N_nodes_max * [-1e9]]])
        for i in range(B):
            for j in range(N_nodes_max):
                A[i, j, j] = 0
        # print(A.shape)
        x = torch.zeros(B, N_nodes_max, C)
        graph_feature = torch.zeros(B, batch[0][3].shape[0])
        for b in range(B):
            x[b, :N_nodes[b]] = batch[b][0]
            A[b, :N_nodes[b], :N_nodes[b]] = batch[b][1]
            graph_support[b][:N_nodes[b]] = 1  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1
            graph_feature[b] = batch[b][3]

        # for i in range(B):
        #     for j in range(N_nodes_max):
        #         for k in range(N_nodes_max):
        #             A[i, j, k] = -1e9 * (1.0 - A[i, j, k])

        # print('done')
        N_nodes = torch.from_numpy(np.array(N_nodes)).long()
        labels = torch.from_numpy(np.array([batch[b][2] for b in range(B)])).long()

        return [x, A, graph_support, N_nodes, labels, graph_feature]

    def load_data(self):  # 加载数据
        if self.args['model'] == 'gcn':  # 如果是GCN模型
            self.load_data_gcn()  # 加载GCN数据
        elif self.args['model'] == 'gat':  # 如果是GAT模型
            self.load_data_gat()  # 加载GAT数据

    def load_data_gat(self):  # 加载GAT数据

        transforms = []  # for PyTorch Geometric 

        print('🥚Loading data')

        self.loss_fn = F.cross_entropy  # 损失函数：交叉熵
        self.predict_fn = lambda output: output.max(1, keepdim=True)[1].detach().cpu()  # 预测函数：输出最大值的索引

        self.acc_folds = []  # 准确率


        self.loaders = []  # 数据加载器
        for split in ['train_val', 'test']:  # 训练集和测试集
            if split == 'train_val':  # 训练集
                # 数据读取器
                datareader = DataReader(data_dir=self.root,  # 数据目录
                                        rnd_state=self.rnd_state,  # 随机种子
                                        use_cont_node_attr=self.args['use_cont_node_attr'],  # 是否使用连续节点属性
                                        train_test='train_val',  # 训练集
                                        use_node_labels=self.args['use_node_labels'])  # 是否使用节点标签
                print('------------------------------')
                print(memory_usage())
                # train set
                gdata = GraphData(datareader=datareader, split='train')
                # print(len(gdata.features_onehot[1]))
                # raise Exception
                print('------------------------------')
                print(memory_usage())
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch)
                print('------------------------------')
                print(memory_usage())
                self.loaders.append(loader)
                print('------------------------------')
                print(memory_usage())
                # for _, data in enumerate(loader):
                #     print(data[5])
                #     print('ALL ok')
                #     break
                # val set
                gdata = GraphData(datareader=datareader, split='val')
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch)
                self.loaders.append(loader)
            else:
                datareader = DataReader(data_dir=self.root,  # 测试集
                                        rnd_state=self.rnd_state,  # 随机种子
                                        use_cont_node_attr=self.args['use_cont_node_attr'],  # 是否使用连续节点属性
                                        train_test='test',  # 测试集
                                        use_node_labels=self.args['use_node_labels'])  # 是否使用节点标签
                # train set
                gdata = GraphData(datareader=datareader, split='test')
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch)
                self.loaders.append(loader)
        print('\nFOLD , train {}, test {}'.format(len(self.loaders[0].dataset), len(self.loaders[1].dataset)))

    def load_data_gcn(self):
        transforms = []  # for PyTorch Geometric

        print('Loading data')
        # raise Exception

        self.loss_fn = F.cross_entropy
        self.predict_fn = lambda output: output.max(1, keepdim=True)[1].detach().cpu()

        self.acc_folds = []


        self.loaders = []
        for split in ['train_val', 'test']:
            if split == 'train_val':
                datareader = DataReaderGCN(data_dir=self.root,
                                           rnd_state=self.rnd_state,
                                           use_cont_node_attr=self.args['use_cont_node_attr'],
                                           train_test='train_val',
                                           use_node_labels=self.args['use_node_labels'])
                print('------------------------------')
                print(memory_usage())
                # train set
                gdata = GraphData(datareader=datareader, split='train')
                # print(len(gdata.features_onehot[1]))
                # raise Exception
                print('------------------------------')
                print(memory_usage())
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch_gcn)
                print('------------------------------')
                print(memory_usage())
                self.loaders.append(loader)
                print('------------------------------')
                print(memory_usage())
                # for _, data in enumerate(loader):
                #     print(data[5])
                #     print('ALL ok')
                #     break
                # val set
                gdata = GraphData(datareader=datareader, split='val')
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch_gcn)
                self.loaders.append(loader)
            else:
                datareader = DataReaderGCN(data_dir=self.root,
                                           rnd_state=self.rnd_state,
                                           use_cont_node_attr=self.args['use_cont_node_attr'],
                                           train_test='test',
                                           use_node_labels=self.args['use_node_labels'])
                # train set
                gdata = GraphData(datareader=datareader, split='test')
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch_gcn)
                self.loaders.append(loader)
        print('\nFOLD , train {}, test {}'.format(len(self.loaders[0].dataset), len(self.loaders[1].dataset)))

    def init_model(self):
        print(self.args['lr'], self.args['dropout'], self.args['bn'], self.args['model'], self.args['num_heads_per_layer'])
        if self.args['model'] == 'gat':

            self.model = GAT(self.args['num_of_layers'], self.args['num_heads_per_layer'], self.args['num_features_per_layer'],
                 add_skip_connection=self.args['add_skip_connection'], bias=self.args['bias'], dropout=self.args['dropout'],
                 layer_type=self.args['layer_type'], log_attention_weights=self.args['log_attention_weights'], bnorm=self.args['bn']).to(self.args['device'])
        elif self.args['model'] == 'gcn':
            self.model = GCN(in_features=self.loaders[0].dataset.num_features,
                        out_features=self.loaders[0].dataset.num_classes,
                        device=self.args['device'],
                        n_hidden=self.args['n_hidden'],
                        filters=self.args['filters'],
                        K=self.args['filter_scale'],
                        bnorm=self.args['bn'],
                        dropout=self.args['dropout'],
                        adj_sq=self.args['adj_sq'],
                        scale_identity=self.args['scale_identity']).to(self.args['device'])
        else:
            raise NotImplementedError(self.args['model'])

        print('\nInitialize model')
        print(self.model)
        train_params = list(filter(lambda p: p.requires_grad, self.model.parameters()))
        print('N trainable parameters:', np.sum([p.numel() for p in train_params]))

        # self.optimizer = optim.Adam(train_params, lr=self.args['lr'], weight_decay=self.args['wd'], betas=(0.5, 0.999))
        self.optimizer = optim.Adam(train_params, lr=self.args['lr'], weight_decay=self.args['wd'], betas=(0.5, 0.999)) 
        self.scheduler = lr_scheduler.MultiStepLR(self.optimizer, self.args['lr_decay_steps'], gamma=0.5)

    def train(self, train_loader, epoch):

        self.model.train()
        start = time.time()
        train_loss, n_samples = 0, 0
        for batch_idx, data in enumerate(train_loader):
            # if epoch == 0 and batch_idx == 0:
            #     print(data[0][0][0])
            for i in range(len(data)):
                data[i] = data[i].to(self.args['device'])
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.loss_fn(output, data[4])
            loss.backward()
            self.optimizer.step()
            time_iter = time.time() - start
            train_loss += loss.item() * len(output)
            n_samples += len(output)
            if batch_idx % self.args['log_interval'] == 0 or batch_idx == len(train_loader) - 1:
                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f} (avg: {:.6f}) \tsec/iter: {:.4f}'.format(
                    epoch + 1, n_samples, len(train_loader.dataset),
                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples,
                    time_iter / (batch_idx + 1)))
        self.scheduler.step()

    def test(self, test_loader, epoch, if_test=True):
        self.model.eval()
        start = time.time()
        test_loss, correct, n_samples = 0, 0, 0
        count_0_0 = 0
        count_0_1 = 0
        count_1_0 = 0
        count_1_1 = 0
        count_batch = 0
        _0_1 = []
        _1_0 = []
        for batch_idx, data in enumerate(test_loader):
            if not if_test:
                return
                # continue
            for i in range(len(data)):
                data[i] = data[i].to(self.args['device'])
            output = self.model(data)
            loss = self.loss_fn(output, data[4], reduction='sum')
            test_loss += loss.item()
            n_samples += len(output)
            pred = self.predict_fn(output)

            for i in range(len(pred)):
                if data[4].detach().cpu()[i] == 0 and pred[i][0] == 0:
                    count_0_0 += 1
                    # _0_1.append(self.args['batch_size'] * count_batch + i)
                elif data[4].detach().cpu()[i] == 0 and pred[i][0] == 1:
                    count_0_1 += 1
                    _0_1.append(self.args['batch_size'] * count_batch + i)
                elif data[4].detach().cpu()[i] == 1 and pred[i][0] == 0:
                    count_1_0 += 1
                    _1_0.append(self.args['batch_size'] * count_batch + i)
                elif data[4].detach().cpu()[i] == 1 and pred[i][0] == 1:
                    count_1_1 += 1
                    # _1_0.append(self.args['batch_size'] * count_batch + i)

            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()
            count_batch += 1
        # if not if_test:
        #     return None
        acc = 100. * correct / n_samples
        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%) \tsec/iter: {:.4f}'.format(
            epoch + 1,
            test_loss / n_samples,
            correct,
            n_samples,
            acc, (time.time() - start) / len(test_loader)))
        print('0->0: {}, 0->1: {}, 1->0: {}, 1->1: {}\n'.format(count_0_0, count_0_1, count_1_0, count_1_1))
        # print('0->1: {}\n1->0: {}\n'.format(_0_1, _1_0))
        return [acc, count_0_0, count_0_1, count_1_0, count_1_1]

    def train_test(self):
        for epoch in range(self.args['epochs']):
            self.train(self.loaders[0], epoch)  # no need to evaluate after each epoch
            # print('Train acc')
            acc_ = self.test(self.loaders[0], epoch, if_test=True)
            # print('Val acc')
            acc__ = self.test(self.loaders[1], epoch, if_test=True)
            # print('Test acc')
            acc = self.test(self.loaders[2], epoch, if_test=(epoch == self.args['epochs'] - 1))

        self.acc_folds.append(acc)
        if acc[0] > 97:
            torch.save(self.model, self.root + 'model_{}_{}_{}_{}_{}_{}.pth'.format(str(acc[0])[:6], str(self.args['lr']), str(self.args['dropout']), str(self.args['bn']), str(self.args['model']), str(self.args['num_heads_per_layer'])))
        print(self.acc_folds)
        print('Test avg acc (+- std): {} ({})'.format(np.mean(np.array(self.acc_folds)[:,0]), np.std(np.array(self.acc_folds)[:,0])))

    def set_para(self, lr=None, dropout=None, bn=None, model=None, num_of_layers=None, num_heads_per_layer=None, num_features_per_layer=None):
        if lr is not None:
            self.args['lr'] = lr
        if dropout is not None:
            self.args['dropout'] = dropout
        if bn is not None:
            self.args['bn'] = bn
        if model is not None:
            self.args['model'] = model
        if num_of_layers is not None:
            self.args['num_of_layers'] = num_of_layers
        if num_heads_per_layer is not None:
            self.args['num_heads_per_layer'] = num_heads_per_layer  # GAT专用
        if num_features_per_layer is not None:
            self.args['num_features_per_layer'] = num_features_per_layer
        for arg in self.args:
            print(arg, self.args[arg])


# 训练和测试图模型，输入模型保存路径，数据集名称，以及模型参数
def graph_model(model_root, dataset_name, lr=None, dropout=None, bn=None, num_of_layers=None, num_heads_per_layer=None, num_features_per_layer=None):
    # 初始化模型
    print('🤣🤣🤣🤣🤣🤣开始训练模型: ' + dataset_name + '🤣🤣🤣🤣🤣🤣')
    model = graph_net(model_root, dataset_name)
    # 设置模型参数
    print('🤣🤣🤣🤣🤣🤣开始设置模型参数🤣🤣🤣🤣🤣🤣')
    model.set_para(lr=lr, dropout=dropout, bn=bn, num_of_layers=num_of_layers, num_heads_per_layer=num_heads_per_layer, num_features_per_layer=num_features_per_layer)
    # 加载数据
    print('🤣🤣🤣🤣🤣🤣开始加载数据🤣🤣🤣🤣🤣🤣')
    model.load_data()
    # 初始化模型
    print('🤣🤣🤣🤣🤣🤣开始初始化模型🤣🤣🤣🤣🤣🤣')
    model.init_model()
    # 训练和测试
    print('🤣🤣🤣🤣🤣🤣开始训练和测试🤣🤣🤣🤣🤣🤣')
    model.train_test()
