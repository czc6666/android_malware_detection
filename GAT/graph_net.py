import matplotlib

matplotlib.use('agg')  # ä½¿ç”¨aggåç«¯ï¼Œé¿å…åœ¨æœåŠ¡å™¨ä¸Šç»˜å›¾
import numpy as np  # çŸ©é˜µè¿ç®—
import time
import torch
import torch.utils  # æ•°æ®åŠ è½½å™¨
import torch.utils.data  # æ•°æ®åŠ è½½å™¨
import torch.nn.functional as F  # æ¿€æ´»å‡½æ•°
import torch.optim as optim  # ä¼˜åŒ–å™¨
import torch.optim.lr_scheduler as lr_scheduler  # å­¦ä¹ ç‡è°ƒåº¦å™¨
from torch.utils.data import DataLoader  # æ•°æ®åŠ è½½å™¨
from .GraphData import GraphData  # å›¾æ•°æ®
from .DataReader_GAT import DataReader  # æ•°æ®è¯»å–å™¨
from .DataReader_GCN import DataReader as DataReaderGCN  # æ•°æ®è¯»å–å™¨
# from .model import GAT, GCN  #, GraphUnet, MGCN
from .model import GAT, GCN  #, GraphUnet, MGCN
import sys

from czc.memory_usage import memory_usage  # å†…å­˜ä½¿ç”¨

print('torchç‰ˆæœ¬: ', torch.__version__)  # æ‰“å°torchç‰ˆæœ¬
class graph_net:
    args = {}

    def __init_args__(self):
        from .model import LayerType  # å¯¼å…¥LayerTypeï¼Œç”¨äºå›¾å±‚ç±»å‹
        self.args['description'] = 'Graph Convolutional Networks'  # æè¿°
        self.args['dataset'] = 'AMD_AndroZoo_test'  # æ•°æ®é›†
        self.args['model'] = 'gat'  # æ¨¡å‹é€‰æ‹©ï¼Œchoices=['gcn', 'gat']
        self.args['lr'] = 0.001  # å­¦ä¹ ç‡
        self.args['lr_decay_steps'] = '5,10,15,20,25,30,35,40,45'  #,50,55,60,65,70,75,80,85,90,95
        self.args['wd'] = 1e-4  # æƒé‡è¡°å‡
        self.args['dropout'] = 0.2  # dropoutç‡
        self.args['filters'] = '128'  # æ¯å±‚è¿‡æ»¤å™¨æ•°é‡
        self.args['filter_scale'] = 1  # è¿‡æ»¤å™¨æ¯”ä¾‹ï¼ˆæ„Ÿå—é‡å¤§å°ï¼‰ï¼Œå¿…é¡»å¤§äº0ï¼›1ä¸ºGCNï¼Œå¤§äº1ä¸ºChebNet
        self.args['n_hidden'] = 0 # å…¨è¿æ¥å±‚åœ¨æœ€åä¸€å±‚å·ç§¯å±‚åçš„éšè—å•å…ƒæ•°é‡
        self.args['n_hidden_edge'] = 32 # è¾¹é¢„æµ‹ç½‘ç»œä¸­å…¨è¿æ¥å±‚çš„éšè—å•å…ƒæ•°é‡
        self.args['epochs'] = 10 # è®­ç»ƒè½®æ•°
        self.args['batch_size'] = 10 # æ‰¹é‡å¤§å°
        self.args['bn'] = False # ä½¿ç”¨æ‰¹å½’ä¸€åŒ–å±‚
        self.args['threads'] = 0 # åŠ è½½æ•°æ®çº¿ç¨‹æ•°
        self.args['log_interval'] = 10 # æ—¥å¿—é—´éš”ï¼ˆæ‰¹æ¬¡æ•°ï¼‰
        self.args['device'] = 'cuda' # è®¾å¤‡é€‰æ‹©ï¼Œchoices=['cuda', 'cpu']
        self.args['seed'] = 1 # éšæœºç§å­
        self.args['shuffle_nodes'] = False # è°ƒè¯•æ—¶æ‰“ä¹±èŠ‚ç‚¹
        self.args['adj_sq'] = False # ä½¿ç”¨A^2ä»£æ›¿Aä½œä¸ºé‚»æ¥çŸ©é˜µ
        self.args['scale_identity'] = False # ä½¿ç”¨2Iä»£æ›¿Iä½œä¸ºè‡ªè¿æ¥
        self.args['visualize'] = False # ä»…ç”¨äºunetï¼šä¿å­˜ä¸€äº›é‚»æ¥çŸ©é˜µå’Œå…¶ä»–æ•°æ®ä¸ºå›¾åƒ
        self.args['use_cont_node_attr'] = True # ä½¿ç”¨è¿ç»­èŠ‚ç‚¹å±æ€§
        self.args['use_node_labels'] = False # ä½¿ç”¨èŠ‚ç‚¹æ ‡ç­¾
        self.args['num_of_layers'] = 1 # GATä¸“ç”¨ï¼Œè¡¨ç¤ºGATå±‚æ•°
        self.args['num_heads_per_layer'] = [2] # GATä¸“ç”¨
        self.args['num_features_per_layer'] = [500, 128] # GATä¸“ç”¨ï¼Œè¡¨ç¤ºæ¯å±‚çš„ç‰¹å¾æ•°é‡ï¼Œç›¸å½“äºGCNçš„filters
        self.args['add_skip_connection'] = True # GATä¸“ç”¨
        self.args['bias'] = True # GATä¸“ç”¨
        self.args['layer_type'] = LayerType.IMP2 # GATä¸“ç”¨
        self.args['log_attention_weights'] = False # GATä¸“ç”¨

        self.args['filters'] = list(map(int, self.args['filters'].split(',')))  # æ¯å±‚è¿‡æ»¤å™¨æ•°é‡
        self.args['lr_decay_steps'] = list(map(int, self.args['lr_decay_steps'].split(',')))  # å­¦ä¹ ç‡è¡°å‡æ­¥æ•°

    def __init__(self, root, dataset):  # åˆå§‹åŒ–
        self.__init_args__()
        self.args['dataset'] = dataset  # æ•°æ®é›†    
        self.root = root  # æ•°æ®ç›®å½•
        # for arg in self.args:
        #     print(arg, self.args[arg])
        torch.backends.cudnn.deterministic = True  # ç¡®å®šæ€§
        torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–
        torch.manual_seed(self.args['seed'])  # éšæœºç§å­
        torch.cuda.manual_seed(self.args['seed'])  # éšæœºç§å­   
        torch.cuda.manual_seed_all(self.args['seed'])  # éšæœºç§å­
        self.rnd_state = np.random.RandomState(self.args['seed'])  # éšæœºç§å­


    def collate_batch(self, batch):
        '''
        é€šè¿‡å¯¹èŠ‚ç‚¹ç‰¹å¾å’Œé‚»æ¥çŸ©é˜µè¿›è¡Œé›¶å¡«å……ï¼ˆzero-paddingï¼‰æ¥åˆ›å»ºç›¸åŒå¤§å°çš„å›¾æ‰¹æ¬¡ï¼ˆbatchï¼‰ã€‚
        å¡«å……çš„å¤§å°å–å†³äºå½“å‰æ‰¹æ¬¡ä¸­çš„æœ€å¤§èŠ‚ç‚¹æ•°ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§èŠ‚ç‚¹æ•°ã€‚
        æ‰¹æ¬¡ä¸­çš„å›¾é€šå¸¸æ¯”æ•°æ®é›†ä¸­æœ€å¤§çš„å›¾å°å¾—å¤šï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•æ•ˆç‡è¾ƒé«˜ã€‚
        å‚æ•°ï¼š
            batch: PyTorch Geometric æ ¼å¼çš„æ‰¹æ¬¡ï¼Œæˆ–è€…æ˜¯ [èŠ‚ç‚¹ç‰¹å¾*æ‰¹æ¬¡å¤§å°, é‚»æ¥çŸ©é˜µ*æ‰¹æ¬¡å¤§å°, æ ‡ç­¾*æ‰¹æ¬¡å¤§å°] çš„åˆ—è¡¨
        è¿”å›å€¼ï¼š
            [èŠ‚ç‚¹ç‰¹å¾, é‚»æ¥çŸ©é˜µ, å›¾æ”¯æŒçŸ©é˜µ, èŠ‚ç‚¹æ•°é‡, æ ‡ç­¾]
        ã€‚
        Creates a batch of same size graphs by zero-padding node features and adjacency matrices up to
        the maximum number of nodes in the CURRENT batch rather than in the entire dataset.
        Graphs in the batches are usually much smaller than the largest graph in the dataset, so this method is fast.
        :param batch: batch in the PyTorch Geometric format or [node_features*batch_size, A*batch_size, label*batch_size]
        :return: [node_features, A, graph_support, N_nodes, label]
        '''
        # assert len(batch) == 1, str(len(batch)) + 'batch must be 1!'
        B = len(batch)
        # print('--------------------------------------------------------', len(batch[0]))
        N_nodes = [len(batch[b][1]) for b in range(B)]
        C = batch[0][0].shape[1]
        N_nodes_max = int(np.max(N_nodes))

        graph_support = torch.zeros(B, N_nodes_max)
        # A = torch.zeros(B, N_nodes_max, N_nodes_max)
        A = torch.full((B, N_nodes_max, N_nodes_max), -1e9)
        # A = torch.tensor(B*[ N_nodes_max * [N_nodes_max * [-1e9]]])
        for i in range(B):
            for j in range(N_nodes_max):
                A[i, j, j] = 0
        # print(A.shape)
        x = torch.zeros(B, N_nodes_max, C)
        graph_feature = torch.zeros(B, batch[0][3].shape[0])
        for b in range(B):
            x[b, :N_nodes[b]] = batch[b][0]
            A[b, :N_nodes[b], :N_nodes[b]] = batch[b][1]
            graph_support[b][:N_nodes[b]] = 1  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1
            graph_feature[b] = batch[b][3]

        # for i in range(B):
        #     for j in range(N_nodes_max):
        #         for k in range(N_nodes_max):
        #             A[i, j, k] = -1e9 * (1.0 - A[i, j, k])

        # print('done')
        N_nodes = torch.from_numpy(np.array(N_nodes)).long()
        labels = torch.from_numpy(np.array([batch[b][2] for b in range(B)])).long()

        return [x, A, graph_support, N_nodes, labels, graph_feature]

    def collate_batch_gcn(self, batch):
        '''
        Creates a batch of same size graphs by zero-padding node features and adjacency matrices up to
        the maximum number of nodes in the CURRENT batch rather than in the entire dataset.
        Graphs in the batches are usually much smaller than the largest graph in the dataset, so this method is fast.
        :param batch: batch in the PyTorch Geometric format or [node_features*batch_size, A*batch_size, label*batch_size]
        :return: [node_features, A, graph_support, N_nodes, label]
        '''
        # assert len(batch) == 1, str(len(batch)) + 'batch must be 1!'
        # raise Exception

        B = len(batch)
        # print('--------------------------------------------------------', len(batch[0]))
        N_nodes = [len(batch[b][1]) for b in range(B)]
        C = batch[0][0].shape[1]
        N_nodes_max = int(np.max(N_nodes))

        graph_support = torch.zeros(B, N_nodes_max)
        A = torch.zeros(B, N_nodes_max, N_nodes_max)
        # A = torch.full((B, N_nodes_max, N_nodes_max), -1e9)
        # A = torch.tensor(B*[ N_nodes_max * [N_nodes_max * [-1e9]]])
        for i in range(B):
            for j in range(N_nodes_max):
                A[i, j, j] = 0
        # print(A.shape)
        x = torch.zeros(B, N_nodes_max, C)
        graph_feature = torch.zeros(B, batch[0][3].shape[0])
        for b in range(B):
            x[b, :N_nodes[b]] = batch[b][0]
            A[b, :N_nodes[b], :N_nodes[b]] = batch[b][1]
            graph_support[b][:N_nodes[b]] = 1  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1
            graph_feature[b] = batch[b][3]

        # for i in range(B):
        #     for j in range(N_nodes_max):
        #         for k in range(N_nodes_max):
        #             A[i, j, k] = -1e9 * (1.0 - A[i, j, k])

        # print('done')
        N_nodes = torch.from_numpy(np.array(N_nodes)).long()
        labels = torch.from_numpy(np.array([batch[b][2] for b in range(B)])).long()

        return [x, A, graph_support, N_nodes, labels, graph_feature]

    def load_data(self):  # åŠ è½½æ•°æ®
        if self.args['model'] == 'gcn':  # å¦‚æœæ˜¯GCNæ¨¡å‹
            self.load_data_gcn()  # åŠ è½½GCNæ•°æ®
        elif self.args['model'] == 'gat':  # å¦‚æœæ˜¯GATæ¨¡å‹
            self.load_data_gat()  # åŠ è½½GATæ•°æ®

    def load_data_gat(self):  # åŠ è½½GATæ•°æ®

        transforms = []  # for PyTorch Geometric 

        print('ğŸ¥šLoading data')

        self.loss_fn = F.cross_entropy  # æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µ
        self.predict_fn = lambda output: output.max(1, keepdim=True)[1].detach().cpu()  # é¢„æµ‹å‡½æ•°ï¼šè¾“å‡ºæœ€å¤§å€¼çš„ç´¢å¼•

        self.acc_folds = []  # å‡†ç¡®ç‡


        self.loaders = []  # æ•°æ®åŠ è½½å™¨
        for split in ['train_val', 'test']:  # è®­ç»ƒé›†å’Œæµ‹è¯•é›†
            if split == 'train_val':  # è®­ç»ƒé›†
                # æ•°æ®è¯»å–å™¨
                datareader = DataReader(data_dir=self.root,  # æ•°æ®ç›®å½•
                                        rnd_state=self.rnd_state,  # éšæœºç§å­
                                        use_cont_node_attr=self.args['use_cont_node_attr'],  # æ˜¯å¦ä½¿ç”¨è¿ç»­èŠ‚ç‚¹å±æ€§
                                        train_test='train_val',  # è®­ç»ƒé›†
                                        use_node_labels=self.args['use_node_labels'])  # æ˜¯å¦ä½¿ç”¨èŠ‚ç‚¹æ ‡ç­¾
                print('------------------------------')
                print(memory_usage())
                # train set
                gdata = GraphData(datareader=datareader, split='train')
                # print(len(gdata.features_onehot[1]))
                # raise Exception
                print('------------------------------')
                print(memory_usage())
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch)
                print('------------------------------')
                print(memory_usage())
                self.loaders.append(loader)
                print('------------------------------')
                print(memory_usage())
                # for _, data in enumerate(loader):
                #     print(data[5])
                #     print('ALL ok')
                #     break
                # val set
                gdata = GraphData(datareader=datareader, split='val')
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch)
                self.loaders.append(loader)
            else:
                datareader = DataReader(data_dir=self.root,  # æµ‹è¯•é›†
                                        rnd_state=self.rnd_state,  # éšæœºç§å­
                                        use_cont_node_attr=self.args['use_cont_node_attr'],  # æ˜¯å¦ä½¿ç”¨è¿ç»­èŠ‚ç‚¹å±æ€§
                                        train_test='test',  # æµ‹è¯•é›†
                                        use_node_labels=self.args['use_node_labels'])  # æ˜¯å¦ä½¿ç”¨èŠ‚ç‚¹æ ‡ç­¾
                # train set
                gdata = GraphData(datareader=datareader, split='test')
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch)
                self.loaders.append(loader)
        print('\nFOLD , train {}, test {}'.format(len(self.loaders[0].dataset), len(self.loaders[1].dataset)))

    def load_data_gcn(self):
        transforms = []  # for PyTorch Geometric

        print('Loading data')
        # raise Exception

        self.loss_fn = F.cross_entropy
        self.predict_fn = lambda output: output.max(1, keepdim=True)[1].detach().cpu()

        self.acc_folds = []


        self.loaders = []
        for split in ['train_val', 'test']:
            if split == 'train_val':
                datareader = DataReaderGCN(data_dir=self.root,
                                           rnd_state=self.rnd_state,
                                           use_cont_node_attr=self.args['use_cont_node_attr'],
                                           train_test='train_val',
                                           use_node_labels=self.args['use_node_labels'])
                print('------------------------------')
                print(memory_usage())
                # train set
                gdata = GraphData(datareader=datareader, split='train')
                # print(len(gdata.features_onehot[1]))
                # raise Exception
                print('------------------------------')
                print(memory_usage())
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch_gcn)
                print('------------------------------')
                print(memory_usage())
                self.loaders.append(loader)
                print('------------------------------')
                print(memory_usage())
                # for _, data in enumerate(loader):
                #     print(data[5])
                #     print('ALL ok')
                #     break
                # val set
                gdata = GraphData(datareader=datareader, split='val')
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch_gcn)
                self.loaders.append(loader)
            else:
                datareader = DataReaderGCN(data_dir=self.root,
                                           rnd_state=self.rnd_state,
                                           use_cont_node_attr=self.args['use_cont_node_attr'],
                                           train_test='test',
                                           use_node_labels=self.args['use_node_labels'])
                # train set
                gdata = GraphData(datareader=datareader, split='test')
                loader = DataLoader(gdata,
                                    batch_size=self.args['batch_size'],
                                    shuffle=split.find('train') >= 0,
                                    num_workers=self.args['threads'],
                                    collate_fn=self.collate_batch_gcn)
                self.loaders.append(loader)
        print('\nFOLD , train {}, test {}'.format(len(self.loaders[0].dataset), len(self.loaders[1].dataset)))

    def init_model(self):
        print(self.args['lr'], self.args['dropout'], self.args['bn'], self.args['model'], self.args['num_heads_per_layer'])
        if self.args['model'] == 'gat':

            self.model = GAT(self.args['num_of_layers'], self.args['num_heads_per_layer'], self.args['num_features_per_layer'],
                 add_skip_connection=self.args['add_skip_connection'], bias=self.args['bias'], dropout=self.args['dropout'],
                 layer_type=self.args['layer_type'], log_attention_weights=self.args['log_attention_weights'], bnorm=self.args['bn']).to(self.args['device'])
        elif self.args['model'] == 'gcn':
            self.model = GCN(in_features=self.loaders[0].dataset.num_features,
                        out_features=self.loaders[0].dataset.num_classes,
                        device=self.args['device'],
                        n_hidden=self.args['n_hidden'],
                        filters=self.args['filters'],
                        K=self.args['filter_scale'],
                        bnorm=self.args['bn'],
                        dropout=self.args['dropout'],
                        adj_sq=self.args['adj_sq'],
                        scale_identity=self.args['scale_identity']).to(self.args['device'])
        else:
            raise NotImplementedError(self.args['model'])

        print('\nInitialize model')
        print(self.model)
        train_params = list(filter(lambda p: p.requires_grad, self.model.parameters()))
        print('N trainable parameters:', np.sum([p.numel() for p in train_params]))

        # self.optimizer = optim.Adam(train_params, lr=self.args['lr'], weight_decay=self.args['wd'], betas=(0.5, 0.999))
        self.optimizer = optim.Adam(train_params, lr=self.args['lr'], weight_decay=self.args['wd'], betas=(0.5, 0.999)) 
        self.scheduler = lr_scheduler.MultiStepLR(self.optimizer, self.args['lr_decay_steps'], gamma=0.5)

    def train(self, train_loader, epoch):

        self.model.train()
        start = time.time()
        train_loss, n_samples = 0, 0
        for batch_idx, data in enumerate(train_loader):
            # if epoch == 0 and batch_idx == 0:
            #     print(data[0][0][0])
            for i in range(len(data)):
                data[i] = data[i].to(self.args['device'])
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.loss_fn(output, data[4])
            loss.backward()
            self.optimizer.step()
            time_iter = time.time() - start
            train_loss += loss.item() * len(output)
            n_samples += len(output)
            if batch_idx % self.args['log_interval'] == 0 or batch_idx == len(train_loader) - 1:
                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f} (avg: {:.6f}) \tsec/iter: {:.4f}'.format(
                    epoch + 1, n_samples, len(train_loader.dataset),
                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples,
                    time_iter / (batch_idx + 1)))
        self.scheduler.step()

    def test(self, test_loader, epoch, if_test=True):
        self.model.eval()
        start = time.time()
        test_loss, correct, n_samples = 0, 0, 0
        count_0_0 = 0
        count_0_1 = 0
        count_1_0 = 0
        count_1_1 = 0
        count_batch = 0
        _0_1 = []
        _1_0 = []
        for batch_idx, data in enumerate(test_loader):
            if not if_test:
                return
                # continue
            for i in range(len(data)):
                data[i] = data[i].to(self.args['device'])
            output = self.model(data)
            loss = self.loss_fn(output, data[4], reduction='sum')
            test_loss += loss.item()
            n_samples += len(output)
            pred = self.predict_fn(output)

            for i in range(len(pred)):
                if data[4].detach().cpu()[i] == 0 and pred[i][0] == 0:
                    count_0_0 += 1
                    # _0_1.append(self.args['batch_size'] * count_batch + i)
                elif data[4].detach().cpu()[i] == 0 and pred[i][0] == 1:
                    count_0_1 += 1
                    _0_1.append(self.args['batch_size'] * count_batch + i)
                elif data[4].detach().cpu()[i] == 1 and pred[i][0] == 0:
                    count_1_0 += 1
                    _1_0.append(self.args['batch_size'] * count_batch + i)
                elif data[4].detach().cpu()[i] == 1 and pred[i][0] == 1:
                    count_1_1 += 1
                    # _1_0.append(self.args['batch_size'] * count_batch + i)

            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()
            count_batch += 1
        # if not if_test:
        #     return None
        acc = 100. * correct / n_samples
        print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%) \tsec/iter: {:.4f}'.format(
            epoch + 1,
            test_loss / n_samples,
            correct,
            n_samples,
            acc, (time.time() - start) / len(test_loader)))
        print('0->0: {}, 0->1: {}, 1->0: {}, 1->1: {}\n'.format(count_0_0, count_0_1, count_1_0, count_1_1))
        # print('0->1: {}\n1->0: {}\n'.format(_0_1, _1_0))
        return [acc, count_0_0, count_0_1, count_1_0, count_1_1]

    def train_test(self):
        for epoch in range(self.args['epochs']):
            self.train(self.loaders[0], epoch)  # no need to evaluate after each epoch
            # print('Train acc')
            acc_ = self.test(self.loaders[0], epoch, if_test=True)
            # print('Val acc')
            acc__ = self.test(self.loaders[1], epoch, if_test=True)
            # print('Test acc')
            acc = self.test(self.loaders[2], epoch, if_test=(epoch == self.args['epochs'] - 1))

        self.acc_folds.append(acc)
        if acc[0] > 97:
            torch.save(self.model, self.root + 'model_{}_{}_{}_{}_{}_{}.pth'.format(str(acc[0])[:6], str(self.args['lr']), str(self.args['dropout']), str(self.args['bn']), str(self.args['model']), str(self.args['num_heads_per_layer'])))
        print(self.acc_folds)
        print('Test avg acc (+- std): {} ({})'.format(np.mean(np.array(self.acc_folds)[:,0]), np.std(np.array(self.acc_folds)[:,0])))

    def set_para(self, lr=None, dropout=None, bn=None, model=None, num_of_layers=None, num_heads_per_layer=None, num_features_per_layer=None):
        if lr is not None:
            self.args['lr'] = lr
        if dropout is not None:
            self.args['dropout'] = dropout
        if bn is not None:
            self.args['bn'] = bn
        if model is not None:
            self.args['model'] = model
        if num_of_layers is not None:
            self.args['num_of_layers'] = num_of_layers
        if num_heads_per_layer is not None:
            self.args['num_heads_per_layer'] = num_heads_per_layer  # GATä¸“ç”¨
        if num_features_per_layer is not None:
            self.args['num_features_per_layer'] = num_features_per_layer
        for arg in self.args:
            print(arg, self.args[arg])


# è®­ç»ƒå’Œæµ‹è¯•å›¾æ¨¡å‹ï¼Œè¾“å…¥æ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œæ•°æ®é›†åç§°ï¼Œä»¥åŠæ¨¡å‹å‚æ•°
def graph_model(model_root, dataset_name, lr=None, dropout=None, bn=None, num_of_layers=None, num_heads_per_layer=None, num_features_per_layer=None):
    # åˆå§‹åŒ–æ¨¡å‹
    print('ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£å¼€å§‹è®­ç»ƒæ¨¡å‹: ' + dataset_name + 'ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£')
    model = graph_net(model_root, dataset_name)
    # è®¾ç½®æ¨¡å‹å‚æ•°
    print('ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£å¼€å§‹è®¾ç½®æ¨¡å‹å‚æ•°ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£')
    model.set_para(lr=lr, dropout=dropout, bn=bn, num_of_layers=num_of_layers, num_heads_per_layer=num_heads_per_layer, num_features_per_layer=num_features_per_layer)
    # åŠ è½½æ•°æ®
    print('ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£å¼€å§‹åŠ è½½æ•°æ®ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£')
    model.load_data()
    # åˆå§‹åŒ–æ¨¡å‹
    print('ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£å¼€å§‹åˆå§‹åŒ–æ¨¡å‹ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£')
    model.init_model()
    # è®­ç»ƒå’Œæµ‹è¯•
    print('ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£å¼€å§‹è®­ç»ƒå’Œæµ‹è¯•ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£')
    model.train_test()
