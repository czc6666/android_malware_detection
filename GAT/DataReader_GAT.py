import os
import numpy as np
import argparse
from os.path import join
import sys

from czc.memory_usage import memory_usage

def split_ids(ids):
    #import random
    n = len(ids)
    train_ids = ids[:int(0.9 * n)]
    #random.shuffle(train_ids)
    test_ids = ids[int(0.9 * n):]

    return train_ids, test_ids



class DataReader():  # æ•°æ®è¯»å–å™¨
    '''

    åè¯ï¼šadjacency matrixï¼šé‚»æ¥çŸ©é˜µï¼ˆadjï¼‰

    Class to read the txt files containing all data of the dataset.
    Should work for any dataset from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets
    ç”¨äºè¯»å–åŒ…å«æ•°æ®é›†æ‰€æœ‰æ•°æ®çš„æ–‡æœ¬æ–‡ä»¶çš„ç±»ã€‚
    åº”è¯¥é€‚ç”¨äºæ¥è‡ª https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets çš„ä»»ä½•æ•°æ®é›†ã€‚

    è®©
    n = èŠ‚ç‚¹æ€»æ•°
    m = è¾¹æ€»æ•°
    N = å›¾è¡¨æ•°é‡
    DS_A.txt ï¼ˆm è¡Œï¼‰ï¼šæ‰€æœ‰å›¾å½¢çš„ç¨€ç–ï¼ˆå—å¯¹è§’çº¿ï¼‰é‚»æ¥çŸ©é˜µï¼Œæ¯è¡Œå¯¹åº”äº ï¼ˆrowï¼Œ colï¼‰ æˆ– ï¼ˆnode_idï¼Œ node_idï¼‰ã€‚æ‰€æœ‰å›¾å½¢éƒ½æ˜¯æ— å‘çš„ã€‚å› æ­¤ï¼ŒDS_A.txt åŒ…å«æ¯ä¸ªè¾¹çš„ä¸¤ä¸ªæ¡ç›®ã€‚
    DS_graph_indicator.txt ï¼ˆn linesï¼‰ï¼šæ‰€æœ‰å›¾çš„æ‰€æœ‰èŠ‚ç‚¹çš„å›¾æ ‡è¯†ç¬¦çš„åˆ—å‘é‡ï¼Œç¬¬ i è¡Œçš„å€¼æ˜¯ i node_idçš„èŠ‚ç‚¹çš„graph_id
    DS_graph_labels.txtï¼ˆN è¡Œï¼‰ï¼šæ•°æ®é›†ä¸­æ‰€æœ‰å›¾å½¢çš„ç±»æ ‡ç­¾ï¼Œç¬¬ i è¡Œä¸­çš„å€¼æ˜¯graph_id i çš„å›¾å½¢çš„ç±»æ ‡ç­¾
    DS_node_labels.txtï¼ˆn è¡Œï¼‰ï¼šèŠ‚ç‚¹æ ‡ç­¾çš„åˆ—å‘é‡ï¼Œç¬¬ i è¡Œçš„å€¼å¯¹åº”äºnode_idä¸º i çš„èŠ‚ç‚¹

    å¦‚æœç›¸åº”çš„ä¿¡æ¯å¯ç”¨ï¼Œåˆ™æœ‰å¯é€‰æ–‡ä»¶ï¼š
    DS_edge_labels.txtï¼ˆm çº¿;ä¸ DS_A_sparse.txt ç›¸åŒï¼‰ï¼šDS_A_sparse.txt ä¸­è¾¹ç¼˜çš„æ ‡ç­¾
    DS_edge_attributes.txtï¼ˆm çº¿;ä¸ DS_A.txt ç›¸åŒï¼‰ï¼šDS_A.txt ä¸­è¾¹ç¼˜çš„å±æ€§
    DS_node_attributes.txtï¼ˆn è¡Œï¼‰ï¼šèŠ‚ç‚¹å±æ€§çŸ©é˜µï¼Œç¬¬ i è¡Œä¸­çš„é€—å·åˆ†éš”å€¼æ˜¯node_id i çš„èŠ‚ç‚¹çš„å±æ€§å‘é‡
    DS_graph_attributes.txtï¼ˆN è¡Œï¼‰ï¼šæ•°æ®é›†ä¸­æ‰€æœ‰å›¾å½¢çš„å›å½’å€¼ï¼Œç¬¬ i è¡Œä¸­çš„å€¼æ˜¯graph_idä¸º i çš„å›¾å½¢çš„å±æ€§
    '''

    def __init__(self,
                 data_dir,  # åŒ…å«æ–‡æœ¬æ–‡ä»¶çš„æ–‡ä»¶å¤¹ folder with txt files 
                 rnd_state=None,  # éšæœºç§å­
                 use_cont_node_attr=False,  # æ˜¯å¦ä½¿ç”¨è¿ç»­èŠ‚ç‚¹å±æ€§
                 train_test='train_val',  # è®­ç»ƒé›†æˆ–æµ‹è¯•é›†
                 use_node_labels=False  # æ˜¯å¦ä½¿ç”¨èŠ‚ç‚¹æ ‡ç­¾
                 # use or not additional float valued node attributes available in some datasets
                 # åœ¨æŸäº›æ•°æ®é›†ä¸­ä½¿ç”¨æˆ–ä¸ä½¿ç”¨é¢å¤–çš„æµ®ç‚¹å€¼èŠ‚ç‚¹å±æ€§ã€‚
                 ):

        self.data_dir = data_dir  # åŒ…å«æ–‡æœ¬æ–‡ä»¶çš„æ–‡ä»¶å¤¹
        self.rnd_state = np.random.RandomState() if rnd_state is None else rnd_state  # éšæœºç§å­
        self.use_cont_node_attr = use_cont_node_attr  # æ˜¯å¦ä½¿ç”¨è¿ç»­èŠ‚ç‚¹å±æ€§
        self.use_node_labels = use_node_labels  # æ˜¯å¦ä½¿ç”¨èŠ‚ç‚¹æ ‡ç­¾
        files = os.listdir(self.data_dir)  # åˆ—å‡ºæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        if train_test == 'train_val':  # è®­ç»ƒé›†
            file_prefix = '_train_'  # æ–‡ä»¶å‰ç¼€
        else:  # æµ‹è¯•é›†
            file_prefix = '_test_'  # æ–‡ä»¶å‰ç¼€
        data = {}  # æ•°æ®å­—å…¸

        # æ£€æŸ¥å¹¶è¯»å– graph_indicator æ–‡ä»¶
        graph_indicator_files = list(filter(lambda f: f.find(file_prefix + 'graph_indicator.txt') >= 0, files))
        if not graph_indicator_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{file_prefix + 'graph_indicator.txt'}")
        nodes, graphs, indexes = self.read_graph_nodes_relations(graph_indicator_files[0])

        # æ£€æŸ¥å¹¶è¯»å– A.txt æ–‡ä»¶
        a_files = list(filter(lambda f: f.find(file_prefix + 'A.txt') >= 0, files))
        if not a_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶ï¼š{file_prefix + 'A.txt'}")
        data['adj_list'] = self.read_graph_adj(a_files[0], nodes, graphs, indexes)


        print('ğŸ”ğŸ”ğŸ”ğŸ”DataReader_GAT.pyï¼šè¯»å–å›¾èŠ‚ç‚¹å…³ç³»')
        nodes, graphs, indexes= self.read_graph_nodes_relations(list(filter(lambda f: f.find(file_prefix + 'graph_indicator.txt') >= 0, files))[0])  # è¯»å–å›¾èŠ‚ç‚¹å…³ç³»
        print('+++++++++++++++++1')
        print(memory_usage())

        # data['adj_list'] = {}

        print('ğŸ”ğŸ”ğŸ”ğŸ”DataReader_GAT.pyï¼šè¯»å–å›¾é‚»æ¥çŸ©é˜µ')
        data['adj_list'] = self.read_graph_adj(list(filter(lambda f: f.find(file_prefix + 'A.txt') >= 0, files))[0], nodes, graphs, indexes) #, list(filter(lambda f: f.find(file_prefix + 'A2.txt') >= 0, files))[0])

        print('+++++++++++++++++2')
        print(memory_usage())

        print('Read adj list done!')  # è¯»å–å›¾é‚»æ¥çŸ©é˜µå®Œæˆ ï¼ˆadjacency matrixï¼‰

        print('ğŸ”ğŸ”ğŸ”ğŸ”DataReader_GAT.pyï¼šè¯»å–å›¾æ ‡ç­¾')
        data['targets'] = np.array(
            self.parse_txt_file(list(filter(lambda f: f.find(file_prefix + 'graph_labels') >= 0, files))[0],
                                line_parse_fn=lambda s: int(float(s.strip()))))
        
        print('+++++++++++++++++3')
        print(memory_usage())

        if self.use_cont_node_attr:
            print('ğŸ”ğŸ”ğŸ”ğŸ”DataReader_GAT.pyï¼šè¯»å–èŠ‚ç‚¹ç‰¹å¾')
            data['attr'] = self.read_node_features(list(filter(lambda f: f.find(file_prefix + 'node_attributes.txt') >= 0, files))[0],
                                                   nodes, graphs,
                                                   fn=lambda s: np.array(list(map(float, s.strip().split(',')))))
            
        print('+++++++++++++++++4')
        print(memory_usage())

        print('Read node attri done!')  # è¯»å–èŠ‚ç‚¹ç‰¹å¾å®Œæˆ

        print('ğŸ”ğŸ”ğŸ”ğŸ”DataReader_GAT.pyï¼šè¯»å–å›¾ç‰¹å¾')
        data['graph_feature'] = np.array(
            self.parse_txt_file(list(filter(lambda f: f.find(file_prefix + 'graph_attributes.txt') >= 0, files))[0],
                                line_parse_fn=lambda s: [float(z) for z in s.split(',')]))
        
        print('+++++++++++++++++5')
        print(memory_usage())

        if self.use_node_labels:
            print('ğŸ”ğŸ”ğŸ”ğŸ”DataReader_GAT.pyï¼šè¯»å–èŠ‚ç‚¹æ ‡ç­¾')
            data['features'] = self.read_node_features(list(filter(lambda f: f.find(file_prefix + 'node_labels.txt') >= 0, files))[0], nodes, graphs, fn=lambda s: int(s.strip()))
        print(len(data['attr']))

        print('ğŸ”ğŸ”ğŸ”ğŸ”DataReader_GAT.pyï¼šåˆ›å»ºç‰¹å¾')
        features, n_edges, degrees = [], [], []
        for sample_id, adj in enumerate(data['adj_list']):
            N = len(adj)  # number of nodes
            if self.use_node_labels:
                assert N == len(data['features'][sample_id]), (N, len(data['features'][sample_id]))
            # if not np.allclose(adj, adj.T):
            #     print(sample_id, 'not symmetric')
            n = np.sum(adj)  # total sum of edges
            # assert n % 2 == 0, n
            # n_edges.append(int(n / 2))  # undirected edges, so need to divide by 2
            n_edges.append(int(n))
            degrees.extend(list(np.sum(adj, 1)))
            if self.use_node_labels:
                features.append(np.array(data['features'][sample_id]))

        print('+++++++++++++++++6')
        print(memory_usage())

        # Create features over graphs as one-hot vectors for each node åœ¨å›¾ä¸Šåˆ›å»ºç‰¹å¾ï¼Œä½œä¸ºæ¯ä¸ªèŠ‚ç‚¹çš„one-hotå‘é‡
        print('ğŸ”ğŸ”ğŸ”ğŸ”DataReader_GAT.pyï¼šåœ¨å›¾å½¢ä¸Šåˆ›å»ºç‰¹å¾ï¼Œä½œä¸ºæ¯ä¸ªèŠ‚ç‚¹çš„å•çƒ­å‘é‡')
        if self.use_node_labels:
            features_all = np.concatenate(features)
            features_min = features_all.min()
            num_features = int(features_all.max() - features_min + 1)  # number of possible values

        print('+++++++++++++++++7')
        print(memory_usage())

        max_degree = np.max(degrees)
        features_onehot = []  # å•çƒ­å‘é‡ç‰¹å¾
        for sample_id, adj in enumerate(data['adj_list']):
            N = adj.shape[0]
            if self.use_node_labels:
                x = data['features'][sample_id]
                feature_onehot = np.zeros((len(x), num_features))
                for node, value in enumerate(x):
                    feature_onehot[node, value - features_min] = 1
            else:
                feature_onehot = np.empty((N, 0))
            if self.use_cont_node_attr:
                feature_attr = np.array(data['attr'][sample_id])
            else:
                feature_attr = np.empty((N, 0))
            degree_onehot = np.empty((N, 0))
            # print(sample_id)
            # print(feature_onehot.shape, feature_attr.shape, degree_onehot.shape)
            node_features = np.concatenate((feature_onehot, feature_attr, degree_onehot), axis=1)
            if node_features.shape[1] == 0:
                # dummy features for datasets without node labels/attributes
                # node degree features can be used instead
                node_features = np.ones((N, 1))
            features_onehot.append(node_features)

        print('+++++++++++++++++8')
        print(memory_usage())

        num_features = features_onehot[0].shape[1]

        shapes = [len(adj) for adj in data['adj_list']]
        labels = data['targets']  # graph class labels
        labels -= np.min(labels)  # to start from 0

        classes = np.unique(labels)
        num_classes = len(classes)

        print('+++++++++++++++++9')
        print(memory_usage())

        if not np.all(np.diff(classes) == 1):
            print('making labels sequential, otherwise pytorch might crash')
            labels_new = np.zeros(labels.shape, dtype=labels.dtype) - 1
            for lbl in range(num_classes):
                labels_new[labels == classes[lbl]] = lbl
            labels = labels_new
            classes = np.unique(labels)
            assert len(np.unique(labels)) == num_classes, np.unique(labels)

        def stats(x):
            return (np.mean(x), np.std(x), np.min(x), np.max(x))

        print('N nodes avg/std/min/max: \t%.2f/%.2f/%d/%d' % stats(shapes))
        print('N edges avg/std/min/max: \t%.2f/%.2f/%d/%d' % stats(n_edges))
        print('Node degree avg/std/min/max: \t%.2f/%.2f/%d/%d' % stats(degrees))
        print('Node features dim: \t\t%d' % num_features)
        print('N classes: \t\t\t%d' % num_classes)
        print('Classes: \t\t\t%s' % str(classes))
        for lbl in classes:
            print('Class %d: \t\t\t%d samples' % (lbl, np.sum(labels == lbl)))

        print('+++++++++++++++++10')
        print(memory_usage())

        N_graphs = len(labels)  # number of samples (graphs) in data
        print(N_graphs)
        print(len(data['adj_list']))
        print(len(features_onehot))
        assert N_graphs == len(data['adj_list']) == len(features_onehot), 'invalid data'

        # Create train/test sets first
        # if train_test == 'train_val':
        train_ids, val_ids = split_ids(rnd_state.permutation(N_graphs))
        # else:
        #     train_ids, val_ids = split_ids(N_graphs)

        print('+++++++++++++++++11')
        print(memory_usage())

        # Create train sets
        splits = {'train': list(train_ids), 'val': list(val_ids)}

        data['features_onehot'] = features_onehot
        data['targets'] = labels
        data['splits'] = splits
        data['N_nodes_max'] = np.max(shapes)  # max number of nodes
        data['num_features'] = num_features
        data['num_classes'] = num_classes

        print('+++++++++++++++++12')
        print(memory_usage())

        self.data = data

    def parse_txt_file(self, fpath, line_parse_fn=None):
        print(fpath)
        data = []
        count = 0
        with open(join(self.data_dir, fpath), 'r') as f:
        #     lines = f.readlines()
        # print('---')
        # data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]
            while True:
                lines = f.readlines(10000)
                if not lines:
                    break
                for line in lines:
                    count += 1
                    if count % 1000000 == 0:
                        print(count)
                    data.append(line_parse_fn(line) if line_parse_fn is not None else line)
        return data
    def parse_txt_file_single(self, fpath, line_parse_fn=None):
        print(fpath)
        data = []
        count = 0
        with open(join(self.data_dir, fpath), 'r') as f:
        #     lines = f.readlines()
        # print('---')
        # data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]
            while True:
                lines = f.readlines(10000)
                if not lines:
                    break
                for line in lines:
                    count += 1
                    if count % 1000000 == 0:
                        print(count)
                    yield line_parse_fn(line) if line_parse_fn is not None else line
        # return data

    # graph key is graph_id, value is list of nodes
    def read_graph_adj(self, fpath, nodes, graphs, indexes, fpath2=None):
        # edges = self.parse_txt_file(fpath, line_parse_fn=lambda s: [int(num.strip()) for num in s.split(',')])
        adj_dict = {}
        # æ·»åŠ æ¯ä¸ªèŠ‚ç‚¹çš„è‡ªç¯
        # for node in nodes:
        #     edges.append((str(node), str(node)))
        # for edge in edges:
        for edge in self.parse_txt_file_single(fpath, line_parse_fn=lambda s: [int(num.strip()) for num in s.split(',')]):
            # print(edge)
            node1 = edge[0]#int(edge[0].strip())# - 1  # -1 because of zero-indexing in our code
            node2 = edge[1]#int(edge[1].strip())# - 1
            graph_id = nodes[node1]
            assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])
            if graph_id not in adj_dict:
                n = len(graphs[graph_id])
                # adj_dict[graph_id] = np.zeros((n, n))
                adj_dict[graph_id] = np.full((n, n), -1e9)
                # adj_dict[graph_id] = np.array(n * [n * [-1e9]])
                # print(adj_dict[graph_id].shape)
                # raise Exception
                # æ·»åŠ æ¯ä¸ªèŠ‚ç‚¹çš„è‡ªç¯
                for i in range(n):
                    adj_dict[graph_id][i, i] = 0
            # ind1 = np.where(graphs[graph_id] == node1)[0]
            # ind2 = np.where(graphs[graph_id] == node2)[0]
            # print(ind2)
            ind1 = np.array([indexes[node1]])
            ind2 = np.array([indexes[node2]])
            # print(ind2)
            assert len(ind1) == len(ind2) == 1, (ind1, ind2)
            adj_dict[graph_id][ind1, ind2] = 0
            # pass

            # è½¬æˆæ— å‘å›¾
            adj_dict[graph_id][ind2, ind1] = 0
        # print(adj_dict[0])
        # æ·»åŠ èŠ‚ç‚¹ç‰¹å¾å…³è”
        # if fpath2 is not None:
        #     for edge in self.parse_txt_file_single(fpath2, line_parse_fn=lambda s: [int(num.strip()) for num in s.split(',')]):
        #
        #         # print(edge)
        #         node1 = edge[0]#int(edge[0].strip())# - 1  # -1 because of zero-indexing in our code
        #         node2 = edge[1]#int(edge[1].strip())# - 1
        #         graph_id = nodes[node1]
        #         assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])
        #         if graph_id not in adj_dict:
        #             n = len(graphs[graph_id])
        #             adj_dict[graph_id] = np.zeros((n, n))
        #         # ind1 = np.where(graphs[graph_id] == node1)[0]
        #         # ind2 = np.where(graphs[graph_id] == node2)[0]
        #         # print(ind2)
        #         ind1 = np.array([indexes[node1]])
        #         ind2 = np.array([indexes[node2]])
        #         # print(ind2)
        #         assert len(ind1) == len(ind2) == 1, (ind1, ind2)
        #         adj_dict[graph_id][ind1, ind2] = 1
        #         pass
        #
        #         # è½¬æˆæ— å‘å›¾
        #         adj_dict[graph_id][ind2, ind1] = 1
        # print(adj_dict[0])
        for graph_id in graphs.keys():
            if graph_id not in adj_dict:
                n = len(graphs[graph_id])
                adj_dict[graph_id] = np.zeros((n, n))
        adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]

        return adj_list

    def read_graph_nodes_relations(self, fpath):  # è¯»å–å›¾èŠ‚ç‚¹å…³ç³»
        graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))  # è¯»å–å›¾èŠ‚ç‚¹å…³ç³»
        nodes, graphs = {}, {}  # èŠ‚ç‚¹å’Œå›¾çš„å­—å…¸
        indexes = [0.0 for i in range(len(graph_ids))]  # ç´¢å¼•åˆ—è¡¨
        for node_id, graph_id in enumerate(graph_ids):  # æšä¸¾å›¾èŠ‚ç‚¹å…³ç³»
            if graph_id not in graphs:  # å¦‚æœå›¾ä¸åœ¨å›¾ä¸­ï¼Œåˆ™æ·»åŠ 
                graphs[graph_id] = []  # æ·»åŠ å›¾
            graphs[graph_id].append(node_id)  # æ·»åŠ èŠ‚ç‚¹
            nodes[node_id] = graph_id  # æ·»åŠ èŠ‚ç‚¹
            indexes[node_id] = len(graphs[graph_id]) - 1  # æ·»åŠ ç´¢å¼•
        graph_ids = np.unique(list(graphs.keys()))  # è·å–å›¾çš„å”¯ä¸€å€¼
        for graph_id in graph_ids:  
            graphs[graph_id] = np.array(graphs[graph_id])
        return nodes, graphs, indexes

    def read_node_features(self, fpath, nodes, graphs, fn):  # è¯»å–èŠ‚ç‚¹ç‰¹å¾
        node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn)
        node_features = {}
        for node_id, x in enumerate(node_features_all):
            graph_id = nodes[node_id]
            if graph_id not in node_features:
                node_features[graph_id] = [None] * len(graphs[graph_id])
            ind = np.where(graphs[graph_id] == node_id)[0]
            assert len(ind) == 1, ind
            assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]
            node_features[graph_id][ind[0]] = x
        node_features_lst = [node_features[graph_id] for graph_id in sorted(list(graphs.keys()))]
        return node_features_lst
